Evaluation Report - 2026-02-10T22:25:00.350Z
Source: /Users/bryceharmon/Desktop/prompt-builder/scripts/evaluation/data/evaluation-prompts-latest.json
Judge Model: gpt-4o

================================================================================
  SPAN LABELING EVALUATION REPORT
================================================================================

üìä SUMMARY (2 prompts evaluated):
  Average Score:      0/25
  Average Span Count: 0
  Errors:             2

üìà SCORE DISTRIBUTION:
  excellent (23-25)     0
  good (18-22)          0
  acceptable (13-17)    0
  poor (8-12)           0
  failing (0-7)         0

‚è±Ô∏è  LATENCY STATS (ms):
  Avg: 0 | P50: 0 | P95: 0 | P99: 0

MISSED ELEMENTS BY SEVERITY (Examples):

CATEGORY SCORES (avg coverage/precision):
  shot         0.00 / 0.00
  subject      0.00 / 0.00
  action       0.00 / 0.00
  environment  0.00 / 0.00
  lighting     0.00 / 0.00
  camera       0.00 / 0.00
  style        0.00 / 0.00
  technical    0.00 / 0.00
  audio        0.00 / 0.00

ERRORS BY SECTION:
  main            missed 0, falsePositives 0
  technicalSpecs  missed 0, falsePositives 0
  alternatives    missed 0, falsePositives 0

CONFIDENCE ERROR RATES:
  high   0.0% (0/0)
  medium 0.0% (0/0)
  low    0.0% (0/0)
  Notes: No false positives to analyze. No clear confidence threshold recommendation.

================================================================================