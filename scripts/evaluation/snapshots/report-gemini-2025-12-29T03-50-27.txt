Evaluation Report - 2025-12-29T03:50:27.925Z
Extraction Model: gemini-2.5-flash
Judge Model: gpt-4o

================================================================================
  GEMINI SPAN LABELING EVALUATION REPORT
================================================================================

üìä SUMMARY (1 prompts evaluated):
  Average Score:      22.00/25
  Average Span Count: 18.00
  Success/Errors:     1/0

üìà SCORE DISTRIBUTION:
  excellent (23-25)     0
  good (18-22)         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 1
  acceptable (13-17)    0
  poor (8-12)           0
  failing (0-7)         0

‚è±Ô∏è  LATENCY STATS (ms):
  Extraction: Avg=9323 | P50=9323 | P95=9323
  Judge:      Avg=3823 | P50=3823 | P95=3823

CATEGORY SCORES (avg coverage/precision):
  shot           5.00 / 5.00
  subject        5.00 / 5.00
  action         4.00 / 5.00
  environment    5.00 / 5.00
  lighting       4.00 / 5.00
  camera         4.00 / 5.00
  style          4.00 / 4.00
  technical      5.00 / 5.00
  audio          5.00 / 5.00

‚ùå COMMONLY MISSED ELEMENTS:
  - handheld tracking from a low angle (1x, camera)
  - soft shadows (1x, lighting)

‚ö†Ô∏è  COMMON FALSE POSITIVES:
  - sports photography clarity (1x, abstract_concept)

üîÑ TOP TAXONOMY ERRORS:
  - action ‚Üí action.movement (1x) e.g. "dribbling a basketball with precision and agility"

üîç WORST PERFORMERS (for debugging):
  [22/25] "Test input..."
    Notes: The extraction was mostly accurate, with some minor issues in coverage and taxonomy. The 'handheld t

================================================================================