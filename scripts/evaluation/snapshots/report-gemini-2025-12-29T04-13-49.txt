Evaluation Report - 2025-12-29T04:13:49.550Z
Extraction Model: gemini-2.5-flash
Judge Model: gpt-4o

================================================================================
  GEMINI SPAN LABELING EVALUATION REPORT
================================================================================

üìä SUMMARY (1 prompts evaluated):
  Average Score:      23.00/25
  Average Span Count: 24.00
  Success/Errors:     1/0

üìà SCORE DISTRIBUTION:
  excellent (23-25)    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 1
  good (18-22)          0
  acceptable (13-17)    0
  poor (8-12)           0
  failing (0-7)         0

‚è±Ô∏è  LATENCY STATS (ms):
  Extraction: Avg=5527 | P50=5527 | P95=5527
  Judge:      Avg=3309 | P50=3309 | P95=3309

CATEGORY SCORES (avg coverage/precision):
  shot           5.00 / 5.00
  subject        5.00 / 5.00
  action         5.00 / 5.00
  environment    5.00 / 5.00
  lighting       5.00 / 5.00
  camera         4.00 / 4.00
  style          5.00 / 5.00
  technical      5.00 / 5.00
  audio          5.00 / 5.00

‚ùå COMMONLY MISSED ELEMENTS:
  - f/4-f/5.6 (1x, camera)

‚ö†Ô∏è  COMMON FALSE POSITIVES:
  - selective focus (f/4-f/5.6) (1x, other)

üîç WORST PERFORMERS (for debugging):
  [23/25] "Test input..."
    Notes: The extraction was mostly accurate, with a minor issue in the camera category where the aperture val

================================================================================