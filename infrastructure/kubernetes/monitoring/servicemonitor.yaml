apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prompt-builder-api
  labels:
    app: prompt-builder
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: prompt-builder
      component: api
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s
      scheme: http
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_name]
          targetLabel: pod
        - sourceLabels: [__meta_kubernetes_namespace]
          targetLabel: namespace
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: node
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prompt-builder-worker
  labels:
    app: prompt-builder
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: prompt-builder
      component: worker
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s
      scheme: http
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_name]
          targetLabel: pod
        - sourceLabels: [__meta_kubernetes_namespace]
          targetLabel: namespace
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: node
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prompt-builder-alerts
  labels:
    app: prompt-builder
    prometheus: kube-prometheus
spec:
  groups:
    - name: prompt-builder.rules
      interval: 30s
      rules:
        # High error rate alert
        - alert: HighErrorRate
          expr: |
            (
              sum(rate(http_requests_total{app="prompt-builder",status=~"5.."}[5m]))
              /
              sum(rate(http_requests_total{app="prompt-builder"}[5m]))
            ) > 0.05
          for: 5m
          labels:
            severity: critical
            component: api
          annotations:
            summary: "High error rate detected"
            description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

        # High latency alert
        - alert: HighLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_milliseconds_bucket{app="prompt-builder"}[5m])) by (le)
            ) > 1000
          for: 5m
          labels:
            severity: warning
            component: api
          annotations:
            summary: "High API latency detected"
            description: "P95 latency is {{ $value }}ms (threshold: 1000ms)"

        # Pod restarts
        - alert: PodRestartingFrequently
          expr: |
            rate(kube_pod_container_status_restarts_total{pod=~"prompt-builder.*"}[15m]) > 0
          for: 5m
          labels:
            severity: warning
            component: api
          annotations:
            summary: "Pod restarting frequently"
            description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes"

        # Memory usage
        - alert: HighMemoryUsage
          expr: |
            (
              sum(container_memory_usage_bytes{pod=~"prompt-builder.*"}) by (pod)
              /
              sum(container_spec_memory_limit_bytes{pod=~"prompt-builder.*"}) by (pod)
            ) > 0.9
          for: 5m
          labels:
            severity: warning
            component: api
          annotations:
            summary: "High memory usage"
            description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"

        # CPU throttling
        - alert: CPUThrottling
          expr: |
            rate(container_cpu_cfs_throttled_seconds_total{pod=~"prompt-builder.*"}[5m]) > 0.3
          for: 10m
          labels:
            severity: warning
            component: api
          annotations:
            summary: "CPU throttling detected"
            description: "Pod {{ $labels.pod }} is being CPU throttled {{ $value | humanizePercentage }} of the time"

        # API availability
        - alert: APIDown
          expr: |
            up{job="prompt-builder-api"} == 0
          for: 2m
          labels:
            severity: critical
            component: api
          annotations:
            summary: "API is down"
            description: "Prompt Builder API has been down for more than 2 minutes"

        # Low success rate
        - alert: LowSuccessRate
          expr: |
            (
              sum(rate(http_requests_total{app="prompt-builder",status=~"2.."}[5m]))
              /
              sum(rate(http_requests_total{app="prompt-builder"}[5m]))
            ) < 0.95
          for: 5m
          labels:
            severity: warning
            component: api
          annotations:
            summary: "Low success rate"
            description: "Success rate is {{ $value | humanizePercentage }} (threshold: 95%)"

        # Cache hit rate
        - alert: LowCacheHitRate
          expr: |
            (
              sum(rate(cache_hits_total{app="prompt-builder"}[5m]))
              /
              sum(rate(cache_requests_total{app="prompt-builder"}[5m]))
            ) < 0.5
          for: 10m
          labels:
            severity: info
            component: cache
          annotations:
            summary: "Low cache hit rate"
            description: "Cache hit rate is {{ $value | humanizePercentage }} (threshold: 50%)"

        # Deployment rollout stuck
        - alert: DeploymentRolloutStuck
          expr: |
            kube_deployment_status_condition{
              condition="Progressing",
              status="false",
              deployment=~"prompt-builder.*"
            } == 1
          for: 15m
          labels:
            severity: warning
            component: deployment
          annotations:
            summary: "Deployment rollout stuck"
            description: "Deployment {{ $labels.deployment }} rollout has been stuck for 15 minutes"

        # Video job terminal failures
        - alert: VideoJobTerminalFailures
          expr: |
            sum(rate(alerts_total{alert="video_job_terminal_failure"}[10m])) > 0
          for: 5m
          labels:
            severity: warning
            component: video-jobs
          annotations:
            summary: "Video jobs are failing terminally"
            description: "Video job terminal failure alerts are firing (rate: {{ $value }} /sec)"

        # DLQ growth proxy (terminal failures write to video_job_dlq)
        - alert: VideoJobDLQGrowth
          expr: |
            increase(alerts_total{alert="video_job_terminal_failure"}[30m]) > 5
          for: 0m
          labels:
            severity: warning
            component: video-jobs
          annotations:
            summary: "Video job DLQ growth detected"
            description: "More than 5 terminal video job failures in 30 minutes."

        # Video job retries spiking
        - alert: VideoJobRetrySpike
          expr: |
            sum(rate(alerts_total{alert="video_job_requeued"}[10m])) > 0.02
          for: 10m
          labels:
            severity: warning
            component: video-jobs
          annotations:
            summary: "Video job retries are spiking"
            description: "Video job requeue rate exceeded threshold (rate: {{ $value }} /sec)"

        # Worker drain timeout on shutdown
        - alert: VideoWorkerDrainTimeout
          expr: |
            increase(alerts_total{alert="video_job_worker_drain_timeout"}[30m]) > 0
          for: 0m
          labels:
            severity: critical
            component: worker
          annotations:
            summary: "Video worker drain timeout occurred"
            description: "A worker shutdown exceeded drain timeout and forced claim release."

        # Sweeper reclaiming stale jobs indicates claim/heartbeat issues
        - alert: VideoJobStaleReclaimSpike
          expr: |
            sum(rate(alerts_total{alert="video_job_sweeper_stale_reclaimed"}[10m])) > 0.01
          for: 10m
          labels:
            severity: warning
            component: video-jobs
          annotations:
            summary: "Video job stale reclaim activity is elevated"
            description: "Sweeper reclaimed stale jobs above expected baseline (rate: {{ $value }} /sec)"

        # Persistence failures indicate durable storage degradation
        - alert: VideoJobPersistenceFailures
          expr: |
            sum(rate(alerts_total{alert="video_job_persistence_failure"}[10m])) > 0
          for: 5m
          labels:
            severity: critical
            component: storage
          annotations:
            summary: "Video job durable persistence failures detected"
            description: "Video persistence failures are occurring (rate: {{ $value }} /sec)"
